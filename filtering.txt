As some of you may already know, some types of Beeminder graphs (e.g. lose-weight) feature a "moving average" line, together with an "aura" that seek to provide smoothed out versions of your data. The motivation behind these were to help you identify various trends and tendencies in your data, without being distracted by daily fluctuations particularly when data is coming from by inherently unpredictable sources such as one's weight.

So far, these two visual features of Beeminder graphs have not been particularly effective and suffered from various inaccuracies and deficiencies as exemplified in https://github.com/beeminder/road/issues/152. In particular, the moving average line (MAV) used a cumulative weighted, "exponential" average of datapoints starting from the beginning, moving towards the end (see https://en.wikipedia.org/wiki/Exponential_smoothing for details). This technique has the unfortunate consuquence that the resulting filtered line is delayed by a significant amount of time, which makes it lag behind the actual data (i.e. it seems to be further right in the graph that the actual data). This was particularly noticeable for monotonic graphs such as do-more where the moving average line was visibly offset from the actual datapoints. This was in conflict with the goals of the MAV feature, which was to provide an accurate (and undelayed) smoothed out version of your data.

The aura, on the other hand, had a different goal (more like, two different goals). First, it was intended to visualize a coarse "envelope" around the entirety of your data so you could see both how your progress has evolved over a long time period (the shape of the aura), as well as how much variability was expected/observed in the data (the thickness of the aura). The second goal of the aura was to provide a crude but hopefully informative estimate of how your data would progress towards the akrasia horizon that is located a week ahead of today. To this end, the aura used a polynomial (of third degree or lower) that best fits to the visible portion of the data. However, since Beeminder goals usually have a wide range of data trends (e.g. variations based on different seasons, motivational levels, life circumstances etc.), such a low-degree polynomial approximation often failed (sometimes miserably) to achieve either one of these goals.

All of these observations led to our recent revision of both the MAV and the aura generation. We describe below the machinery behind the new versions, and hopefully how they address these concerns.

The algorithms we use to generate the updated versions of these features is based on filtering methods in singal processing. In particular, there are two types of commonly used digital filters, Finite-Impulse-Response (FIR) (https://en.wikipedia.org/wiki/Finite_impulse_response) and Infinite-Impulse-Response (IIR) (https://en.wikipedia.org/wiki/Infinite_impulse_response) filters. FIR filters are akin to moving average filters with a finite window, examples of which are, for example, very common in image filters such as blurring. IIR filters, on the other hand, are closer to the exponential filter mentioned above, beloging to a more general class of recursive filters wherein incoming data is passed through a discrete dynamical system taking past and present values of both the data and the filter states to compute the next filtered data point. Both of these alternatives have pros and cons with respect to computational efficiency anf accuracy, but it turns out that the faster IIR filters were more appropriate for our use cases.

In signal processing, such filters are characterized with their "frequency response", which is how much of various frequency components of the original signal they let through to their output, and how much delay they introduce to each of these frequencies. A useful analogy is with "equalizers" in sound systems wherein one can increase or decrease high, low or middle range frequencies in a sound signal to elininate "noise". That is in fact very informative for what we do, since both the MAV and the aura should be obtained by eliminating high-frequency changes (i.e. daily fluctuations) and show low frequency components (week-to-week or month-to-month trends). Consequently, what we need are so-called "low-pass" filters, which allow low frequency components in the data "signal" and block high frequency components.

Low-pass filters are parameterized primarily by their "cutoff frequency", which is the frequency threshold beeyond which they don't let any components through. In our updated algorithms, we choose a cutoff frequency such that data components which fluctuate within a 20 day window are suppressed. For the aura, we choose a larger fluctuation period (hence a smaller cutoff frequency) of 20 days to 500 days, dynamically chosen with the number of visible data points to optimize its visual appearance and utility. The result is what you see in the following example graphs:

[a few examples of MAV and the aura]

There were also a key technical issues we had to address in getting these graphs. The first fundamental problem is associated with the fact that both FIR and IIR filters introduce what is called a "group delay" on the input, such that the output signal always lags behind the input signal. This was preciselhy the problem with the original MAV line. One cannot eliminate this issue in signal processing applications where your output cannot depend on *future* data (i.e. they have to be causal), but since Beeminder has all your data (all your data belong to us!), we do not have to abide by causality and can apply the same filter once forwards (introducing a delay of +D) and once backwards (with a delay of -D), resulting in an output signal with no delay! This is how it has been possible to make the MAV line much more closely follow your data compared to its performance before.

The second fundamental issue that plagued the use of these filters was "initialization". Since IIR filters have internal state, they need to be properly initialized to make sure that the initial parts of the output also closely mirror the input. The need to also apply the filter backwards makes this more challenging since the last part of the data becomes the "initial state" for this backwards application. Our solution to this problem was to first shift all data points with an affine function so that they start and end at zero (i.e. nd[i] = d[i] - (d[0]+i/N*(d[N]-d[0]))), and add a sufficient number of zeroes to the end of this data sequence to ensure a smooth ending for the filtered signal. Following filtering (i.e. ndf = iir_filter(nd)), original data values were recovered by applying the inverse of this adjustment (i.e. df[i] = ndf[i] + (d[0]+i/N*(d[N]-d[0]))).

Finally, a small but important requirement for the applicability of these filters is that data needs to be uniformly sampled (i.e. one data point every day), which is of course not always true for Beeminde goals. We remedy this small problem by using linear interpolation to fill in the gaps between successive data points which are separated by more than one day. 
